{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How would you approach designing a fair and unbiased algorithm for evaluating job applicants, considering factors like inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing a fair and unbiased algorithm for evaluating job applicants is a multifaceted challenge that requires careful consideration of various aspects, including bias mitigation, representation, and equitable outcomes. Here’s a structured approach to tackle this problem:\n",
       "\n",
       "### 1. **Define Criteria for Evaluation**\n",
       "\n",
       "   - **Identify Job Requirements:** Clearly outline the key competencies, skills, and experiences that are essential for the position. This should be based on data-driven insights and organizational needs.\n",
       "   - **Performance Metrics:** Establish what success looks like in the role, using historical performance data and stakeholder input.\n",
       "\n",
       "### 2. **Data Collection and Representation**\n",
       "\n",
       "   - **Diverse Data Sources:** Collect a comprehensive dataset that includes applicants from various backgrounds. Ensure representation of different demographic groups (gender, race, socioeconomic status, etc.).\n",
       "   - **Bias Audit of Existing Data:** Analyze historical hiring data to identify any inherent biases in past hiring practices. This includes examining patterns that may disadvantage certain groups.\n",
       "\n",
       "### 3. **Bias Mitigation**\n",
       "\n",
       "   - **Blind Recruitment:** Implement anonymized applications where potentially biased information (names, addresses, graduation institutions) is removed to focus on qualifications and experience.\n",
       "   - **Algorithm Fairness Techniques:** Utilize techniques like re-weighting, adversarial debiasing, and fairness constraints to optimize the algorithm for equitable outcomes.\n",
       "   - **Regular Audits:** Establish a schedule for auditing the algorithm’s outcomes for any signs of bias against protected classes.\n",
       "\n",
       "### 4. **Algorithm Design**\n",
       "\n",
       "   - **Explainable AI:** Choose machine learning models that are interpretable. This allows stakeholders to understand decision-making processes and provides insights into potential biases.\n",
       "   - **Multi-modal Input:** Consider various evaluation methods (technical assessments, interviews, role plays) to capture the full range of candidate attributes.\n",
       "\n",
       "### 5. **Incorporating Human Judgment**\n",
       "\n",
       "   - **Hybrid Approach:** Use the algorithm in conjunction with human judgment. Recruiters can have the final say, particularly in culturally sensitive or nuanced evaluations.\n",
       "   - **Training for Recruiters:** Provide training on unconscious bias, diversity, and inclusion, and how to interpret algorithm outputs critically.\n",
       "\n",
       "### 6. **Feedback Loop and Continuous Improvement**\n",
       "\n",
       "   - **Iterative Testing:** Regularly test the algorithm against real-world outcomes and adjust as necessary. Monitor how well the algorithm predicts on-the-job success for different groups.\n",
       "   - **Stakeholder Feedback:** Create channels for feedback from candidates and hiring managers to understand the experience and gather qualitative data.\n",
       "\n",
       "### 7. **Transparency and Accountability**\n",
       "\n",
       "   - **Clear Communication:** Keep all stakeholders informed about how the algorithm is designed, the data it uses, and how decisions are made.\n",
       "   - **Metrics for Success:** Define success metrics that reflect both hiring goals and diversity/inclusion objectives, and report on these metrics publicly.\n",
       "\n",
       "### 8. **Legal and Ethical Considerations**\n",
       "\n",
       "   - **Compliance:** Ensure the algorithm complies with all relevant laws and regulations (e.g., EEOC guidelines in the U.S.) regarding hiring practices and fairness.\n",
       "   - **Ethical Standards:** Establish and adhere to ethical principles guiding the use of AI in hiring, prioritizing fairness, accountability, and transparency.\n",
       "\n",
       "By employing a comprehensive approach that encompasses data integrity, algorithm fairness, human involvement, and continuous improvement, it’s possible to design an innovative and equitable job applicant evaluation system. This process requires collaboration across stakeholders, ongoing assessment, and a commitment to learning from outcomes to continuously refine methods for inclusivity and fairness."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mclaude-3-7-sonnet-latest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m claude = Anthropic()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mclaude\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m answer = response.content[\u001b[32m0\u001b[39m].text\n\u001b[32m      9\u001b[39m display(Markdown(answer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:283\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/anthropic/resources/messages/messages.py:978\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    972\u001b[39m     warnings.warn(\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    974\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    975\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1314\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1302\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1309\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1310\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1311\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1312\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1313\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1102\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1099\u001b[39m             err.response.read()\n\u001b[32m   1101\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing a fair and unbiased algorithm for evaluating job applicants is a complex task requiring a multi-faceted approach. Here's a breakdown of how I would tackle it, focusing on minimizing biases and promoting equitable outcomes:\n",
       "\n",
       "**1. Define Fairness & Success Metrics Clearly:**\n",
       "\n",
       "*   **Explicitly Define Fairness:**  The first step is to move beyond vague concepts of fairness and define *what fairness means in this specific context*. This involves stakeholder input (HR, hiring managers, employees, potential applicants).  Examples of fairness definitions include:\n",
       "    *   **Demographic Parity (Statistical Parity):**  The selection rate should be approximately the same for different demographic groups. This is a common, but potentially controversial, metric.\n",
       "    *   **Equal Opportunity:**  Individuals with similar qualifications should have an equal chance of being selected, regardless of demographic group.  This requires defining \"qualification\" carefully.\n",
       "    *   **Predictive Parity:**  The algorithm's predictions should be equally accurate across different groups. For example, the rate of correctly identifying qualified candidates should be the same for all groups.\n",
       "    *   **Counterfactual Fairness:** An individual should receive the same outcome had they belonged to a different demographic group. This is a more advanced and computationally expensive approach.\n",
       "\n",
       "*   **Define Success Metrics:**  Clearly define what constitutes \"success\" in the role.  These metrics should be measurable and relevant to job performance. They should also be validated to ensure they are not inherently biased (e.g., relying on communication styles that favor certain cultural backgrounds).\n",
       "\n",
       "**2. Data Acquisition & Preprocessing - Focus on Minimizing Bias:**\n",
       "\n",
       "*   **Data Audit:** Conduct a thorough audit of existing historical hiring data. Identify potential sources of bias, such as:\n",
       "    *   **Selection Bias:** The data used to train the algorithm only reflects those who *were* hired, not those who *could have been* successful.\n",
       "    *   **Label Bias:**  Performance evaluations or other labels might be biased, reflecting existing biases in the organization.\n",
       "    *   **Measurement Bias:** The way data is collected or measured might systematically disadvantage certain groups.  (e.g., using tests written primarily for one culture)\n",
       "\n",
       "*   **Data Augmentation/Balancing:**\n",
       "    *   **Oversampling/Undersampling:**  If certain demographic groups are underrepresented in the training data, oversample them or undersample the overrepresented groups.  However, be careful not to introduce synthetic data that perpetuates stereotypes.\n",
       "    *   **Synthetic Data Generation:** Consider creating synthetic data that fills in gaps in the data and mitigates biases, especially when demographic data is sparse.  Use techniques like Generative Adversarial Networks (GANs) carefully to avoid amplifying existing biases.\n",
       "\n",
       "*   **Feature Selection & Engineering:**\n",
       "    *   **Blind the Algorithm:**  Remove or anonymize protected attributes (e.g., gender, race, age, zip code) from the data used to train the model.  However, be aware of *proxy variables* (features that are highly correlated with protected attributes).\n",
       "    *   **Focus on Job-Relevant Skills:**  Prioritize features that directly reflect skills and experience needed for the job, and de-emphasize features that are less relevant or potentially discriminatory (e.g., hobbies, school prestige).\n",
       "    *   **Consider Alternative Credentials:**  Value skills and experience gained through alternative pathways (e.g., online courses, bootcamps, volunteer work) to create a more inclusive talent pool.\n",
       "    *   **Debias Feature Representations:** Use techniques like adversarial debiasing or word embedding debiasing to reduce biases in feature representations, especially when dealing with textual data (e.g., resumes, cover letters).\n",
       "\n",
       "**3. Algorithm Selection & Development:**\n",
       "\n",
       "*   **Choose Appropriate Algorithms:**  Certain algorithms are more prone to bias than others. For example, highly complex models (like deep neural networks) can easily learn and amplify biases present in the data.  Consider using simpler, more transparent models, especially if explainability is important.\n",
       "*   **Regularization Techniques:**  Apply regularization techniques (e.g., L1 or L2 regularization) to prevent the model from overfitting to biased patterns in the data.\n",
       "*   **Adversarial Debiasing:** Train the algorithm to *minimize* its ability to predict protected attributes while *maximizing* its ability to predict job performance.\n",
       "*   **Ensemble Methods:**  Combine multiple models trained on different subsets of the data or with different debiasing techniques to reduce the overall bias.\n",
       "\n",
       "**4. Model Evaluation & Validation - Rigorous Bias Auditing:**\n",
       "\n",
       "*   **Bias Auditing:** Continuously monitor the algorithm's performance for biases across different demographic groups.  Use metrics like:\n",
       "    *   **Disparate Impact:**  Calculate the selection rate for different groups and compare them to see if there is a statistically significant difference.  (Often measured using the 80% rule).\n",
       "    *   **False Positive/Negative Rates:**  Analyze whether the algorithm makes more errors for certain groups (e.g., falsely rejecting qualified candidates from underrepresented groups).\n",
       "    *   **Calibration Analysis:**  Assess whether the algorithm's predicted probabilities accurately reflect the true probability of success for different groups.\n",
       "*   **A/B Testing:**  Compare the performance of the algorithm to a traditional hiring process using A/B testing to evaluate its impact on diversity and inclusion.\n",
       "*   **Explainable AI (XAI):**  Use XAI techniques (e.g., SHAP values, LIME) to understand which features are driving the algorithm's decisions and identify potential sources of bias.  This allows for a more transparent and accountable system.\n",
       "\n",
       "**5. Implementation & Ongoing Monitoring:**\n",
       "\n",
       "*   **Transparency & Explainability:**  Provide applicants with clear and understandable explanations of how the algorithm works and how their application was evaluated.\n",
       "*   **Human Oversight:**  Don't rely solely on the algorithm.  Incorporate human review at key stages of the hiring process to ensure fairness and address any potential biases.  Provide training to hiring managers on how to interpret the algorithm's results and avoid perpetuating biases.\n",
       "*   **Regular Monitoring & Re-Training:**  Continuously monitor the algorithm's performance and re-train it periodically with updated data to ensure it remains fair and accurate.\n",
       "*   **Feedback Mechanisms:**  Establish a process for applicants to provide feedback on the hiring process, including the algorithm.\n",
       "*   **Ethical Review Board:**  Establish an ethical review board to oversee the development and deployment of the algorithm and ensure it aligns with the organization's values.\n",
       "\n",
       "**Important Considerations:**\n",
       "\n",
       "*   **Context Matters:**  The specific techniques used to mitigate bias will depend on the specific job, the data available, and the organization's values.\n",
       "*   **No Perfect Solution:**  It is impossible to eliminate all bias from an algorithm. The goal is to minimize bias as much as possible and ensure that the algorithm is fair and equitable.\n",
       "*   **Legal Compliance:**  Ensure that the algorithm complies with all applicable laws and regulations regarding discrimination.\n",
       "*   **Continuous Improvement:**  Fairness is an ongoing process, not a one-time fix. Continuously monitor the algorithm's performance and adapt it as needed to address any emerging biases.\n",
       "\n",
       "By following these steps, you can create an algorithm that is more fair and unbiased than a traditional hiring process, and that promotes equitable outcomes for all applicants. It requires a commitment to ethical considerations, data analysis, and ongoing monitoring. Remember to always prioritize fairness and transparency throughout the entire process.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIStatusError",
     "evalue": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIStatusError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m deepseek = OpenAI(api_key=deepseek_api_key, base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.deepseek.com/v1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mdeepseek-chat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43mdeepseek\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m answer = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m      7\u001b[39m display(Markdown(answer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAPIStatusError\u001b[39m: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing a fair and unbiased algorithm for evaluating job applicants requires a multidisciplinary approach, involving expertise in machine learning, psychology, sociology, and ethics. Here's a comprehensive framework to help you create a fair and equitable algorithm:\n",
       "\n",
       "**Pre-design phase**\n",
       "\n",
       "1. **Define the problem and goals**: Identify the specific job requirements, the type of applicants you're looking for, and the desired outcomes. Ensure that the goals are aligned with the organization's values and diversity, equity, and inclusion (DEI) policies.\n",
       "2. **Assemble a diverse team**: Bring together a team with diverse backgrounds, expertise, and perspectives to co-design the algorithm. This team should include data scientists, psychologists, sociologists, ethicists, and HR professionals.\n",
       "3. **Conduct a thorough literature review**: Research existing algorithms, biases, and fairness metrics to understand the current state of the field.\n",
       "\n",
       "**Data collection and preprocessing**\n",
       "\n",
       "1. **Collect diverse and representative data**: Gather data from various sources, including job applications, resumes, cover letters, and assessments. Ensure that the data is representative of the diverse pool of applicants you're targeting.\n",
       "2. **Remove identifiable information**: Anonymize the data by removing identifiable information such as names, ages, addresses, and other demographic characteristics that could introduce biases.\n",
       "3. **Preprocess data**: Clean, transform, and normalize the data to prepare it for analysis.\n",
       "\n",
       "**Algorithm development**\n",
       "\n",
       "1. **Select a suitable algorithm**: Choose an algorithm that is transparent, explainable, and auditable, such as a decision tree or a logistic regression model.\n",
       "2. **Use fairness metrics**: Integrate fairness metrics, such as demographic parity, equal opportunity, or calibration, to evaluate the algorithm's performance and identify potential biases.\n",
       "3. **Regularization techniques**: Implement regularization techniques, such as L1 or L2 regularization, to reduce overfitting and prevent the algorithm from relying too heavily on any single feature.\n",
       "4. **Feature selection**: Select features that are relevant to the job requirements and minimally correlated with protected characteristics (e.g., age, sex, ethnicity).\n",
       "5. **Model interpretability**: Ensure that the algorithm is interpretable, allowing for the identification of features that contribute to the decision-making process.\n",
       "\n",
       "**Bias detection and mitigation**\n",
       "\n",
       "1. **Bias detection tools**: Utilize tools, such as fairness metrics, bias detection algorithms, or audits, to identify potential biases in the algorithm.\n",
       "2. **Debiasing techniques**: Apply debiasing techniques, such as data preprocessing, feature engineering, or regularization, to mitigate identified biases.\n",
       "3. **Human oversight**: Implement human oversight and review processes to detect and correct potential biases.\n",
       "\n",
       "**Evaluation and validation**\n",
       "\n",
       "1. **Evaluate the algorithm**: Assess the algorithm's performance using fairness metrics, accuracy, and other relevant metrics.\n",
       "2. **Validate the results**: Validate the algorithm's results by comparing them to human evaluations or other fairness metrics.\n",
       "3. **Iterate and refine**: Refine the algorithm based on the evaluation and validation results, and iterate until the desired level of fairness and accuracy is achieved.\n",
       "\n",
       "**Deployment and monitoring**\n",
       "\n",
       "1. **Deploy the algorithm**: Deploy the algorithm in a controlled environment, with clear guidelines and protocols for its use.\n",
       "2. **Monitor performance**: Continuously monitor the algorithm's performance, fairness, and accuracy, and update it as needed.\n",
       "3. **Transparency and explainability**: Provide transparency and explainability of the algorithm's decisions, ensuring that applicants and stakeholders understand the evaluation process.\n",
       "\n",
       "**Addressing inherent cognitive biases**\n",
       "\n",
       "1. **Awareness and education**: Educate the development team and users about inherent cognitive biases, such as confirmation bias, anchoring bias, or affinity bias.\n",
       "2. **Design for diversity**: Incorporate diverse perspectives and ideas into the algorithm's design to minimize the impact of cognitive biases.\n",
       "3. **Regular audits**: Conduct regular audits to detect and address potential biases introduced by cognitive biases.\n",
       "\n",
       "**Diverse backgrounds and equitable outcomes**\n",
       "\n",
       "1. **Inclusive design**: Design the algorithm to be inclusive of diverse backgrounds, cultures, and perspectives.\n",
       "2. **Equitable outcomes**: Ensure that the algorithm prioritizes equitable outcomes, such as equal opportunities for underrepresented groups.\n",
       "3. **Continuous improvement**: Continuously monitor and improve the algorithm to ensure that it promotes equitable outcomes and minimizes biases.\n",
       "\n",
       "By following this framework, you can develop a fair and unbiased algorithm for evaluating job applicants, taking into account inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Remember that fairness and bias are ongoing concerns, and continuous monitoring and improvement are essential to maintaining a fair and equitable algorithm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing a fair and unbiased algorithm for evaluating job applicants requires a multi-faceted approach that addresses inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Here's a step-by-step guide to help you create an algorithm that minimizes bias and ensures fairness:\n",
       "\n",
       "1. **Define clear goals and criteria**: Establish specific evaluation criteria and goals for each position. Ensure these are measurable, objective, and relevant to the job requirements.\n",
       "2. **Data collection and preprocessing**:\n",
       "\t* Gather data on applicants, including their resumes, cover letters, social media profiles, and any other relevant information.\n",
       "\t* Preprocess the data by removing irrelevant information, normalizing data formats, and converting text data into numerical representations (e.g., sentiment analysis).\n",
       "3. **Identify bias-reducing techniques**:\n",
       "\t* Use de-biased resume screening tools that focus on relevant skills and qualifications rather than demographic information.\n",
       "\t* Implement algorithms that detect and mitigate biases in natural language processing (NLP) tasks, such as sentiment analysis and named entity recognition.\n",
       "4. **Mitigate inherent cognitive biases**:\n",
       "\t* Use techniques like randomization or stratification to ensure diverse samples of applicants are evaluated.\n",
       "\t* Implement blind hiring practices, where the algorithm is not trained on identifiable information (e.g., names, ages).\n",
       "5. **Incorporate diversity and inclusion metrics**:\n",
       "\t* Monitor and track data on applicant diversity, including demographics, education levels, and work experience.\n",
       "\t* Use algorithms that recognize and reward diversity in the evaluation process.\n",
       "6. **Algorithmic audit and testing**: Regularly review and test your algorithm to ensure it is free from bias and produces equitable results. You can use techniques like:\n",
       "\t* Bias detection tools (e.g., AI Fairness 360, Demographic Prediction Tool)\n",
       "\t* Simulations of diverse scenarios to test the algorithm's fairness\n",
       "7. **Regular evaluation and adaptation**:\n",
       "\t* Continuously monitor the performance of your algorithm and make adjustments as needed.\n",
       "\t* Include regular review of diversity metrics to ensure the pipeline is fair and inclusive.\n",
       "\n",
       "Best Practices for Developing an Algorithmic Ecosystem:\n",
       "\n",
       "1.  **Engage diverse stakeholders**: Involve HR, diversity experts, and representatives from underrepresented groups in the design and development process.\n",
       "2.  **Establish metrics for fairness**: Use universally accepted metrics (e.g., accuracy metrics) to measure your algorithm's performance and biases.\n",
       "3.  **Regularly review data quality**: Ensure high-quality data is used as input for decision-making.\n",
       "4.  **Implement feedback mechanisms**: Create an open platform for applicants to provide feedback on the hiring process.\n",
       "\n",
       "By incorporating these best practices, strategies, and algorithms that mitigate bias, you can develop a fair and unbiased algorithm that advances diversity and inclusion in your organization's hiring practices.\n",
       "\n",
       "**Example: AI Model**\n",
       "\n",
       "Here's an algorithm to help evaluate job applicants using AI models:\n",
       "\n",
       "*   **Step 1:** Preprocess application data by normalizing skills, education level, and other relevant information into numerical representations.\n",
       "*   **Step 2:** Train a machine learning (ML) model on the curated preprocessed data, which can identify relevant qualities, skills, and experience in job seekers.\n",
       "*   **Key Metric**:\n",
       "\n",
       "    *   90% accuracy rate for predicting candidate's ability to fit company culture\n",
       "    *   60% diversity bonus when including applicants underrepresented groups"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'gemini-2.0-flash', 'llama-3.3-70b-versatile', 'llama3.2']\n",
      "['Designing a fair and unbiased algorithm for evaluating job applicants is a multifaceted challenge that requires careful consideration of various aspects, including bias mitigation, representation, and equitable outcomes. Here’s a structured approach to tackle this problem:\\n\\n### 1. **Define Criteria for Evaluation**\\n\\n   - **Identify Job Requirements:** Clearly outline the key competencies, skills, and experiences that are essential for the position. This should be based on data-driven insights and organizational needs.\\n   - **Performance Metrics:** Establish what success looks like in the role, using historical performance data and stakeholder input.\\n\\n### 2. **Data Collection and Representation**\\n\\n   - **Diverse Data Sources:** Collect a comprehensive dataset that includes applicants from various backgrounds. Ensure representation of different demographic groups (gender, race, socioeconomic status, etc.).\\n   - **Bias Audit of Existing Data:** Analyze historical hiring data to identify any inherent biases in past hiring practices. This includes examining patterns that may disadvantage certain groups.\\n\\n### 3. **Bias Mitigation**\\n\\n   - **Blind Recruitment:** Implement anonymized applications where potentially biased information (names, addresses, graduation institutions) is removed to focus on qualifications and experience.\\n   - **Algorithm Fairness Techniques:** Utilize techniques like re-weighting, adversarial debiasing, and fairness constraints to optimize the algorithm for equitable outcomes.\\n   - **Regular Audits:** Establish a schedule for auditing the algorithm’s outcomes for any signs of bias against protected classes.\\n\\n### 4. **Algorithm Design**\\n\\n   - **Explainable AI:** Choose machine learning models that are interpretable. This allows stakeholders to understand decision-making processes and provides insights into potential biases.\\n   - **Multi-modal Input:** Consider various evaluation methods (technical assessments, interviews, role plays) to capture the full range of candidate attributes.\\n\\n### 5. **Incorporating Human Judgment**\\n\\n   - **Hybrid Approach:** Use the algorithm in conjunction with human judgment. Recruiters can have the final say, particularly in culturally sensitive or nuanced evaluations.\\n   - **Training for Recruiters:** Provide training on unconscious bias, diversity, and inclusion, and how to interpret algorithm outputs critically.\\n\\n### 6. **Feedback Loop and Continuous Improvement**\\n\\n   - **Iterative Testing:** Regularly test the algorithm against real-world outcomes and adjust as necessary. Monitor how well the algorithm predicts on-the-job success for different groups.\\n   - **Stakeholder Feedback:** Create channels for feedback from candidates and hiring managers to understand the experience and gather qualitative data.\\n\\n### 7. **Transparency and Accountability**\\n\\n   - **Clear Communication:** Keep all stakeholders informed about how the algorithm is designed, the data it uses, and how decisions are made.\\n   - **Metrics for Success:** Define success metrics that reflect both hiring goals and diversity/inclusion objectives, and report on these metrics publicly.\\n\\n### 8. **Legal and Ethical Considerations**\\n\\n   - **Compliance:** Ensure the algorithm complies with all relevant laws and regulations (e.g., EEOC guidelines in the U.S.) regarding hiring practices and fairness.\\n   - **Ethical Standards:** Establish and adhere to ethical principles guiding the use of AI in hiring, prioritizing fairness, accountability, and transparency.\\n\\nBy employing a comprehensive approach that encompasses data integrity, algorithm fairness, human involvement, and continuous improvement, it’s possible to design an innovative and equitable job applicant evaluation system. This process requires collaboration across stakeholders, ongoing assessment, and a commitment to learning from outcomes to continuously refine methods for inclusivity and fairness.', 'Designing a fair and unbiased algorithm for evaluating job applicants is a complex task requiring a multi-faceted approach. Here\\'s a breakdown of how I would tackle it, focusing on minimizing biases and promoting equitable outcomes:\\n\\n**1. Define Fairness & Success Metrics Clearly:**\\n\\n*   **Explicitly Define Fairness:**  The first step is to move beyond vague concepts of fairness and define *what fairness means in this specific context*. This involves stakeholder input (HR, hiring managers, employees, potential applicants).  Examples of fairness definitions include:\\n    *   **Demographic Parity (Statistical Parity):**  The selection rate should be approximately the same for different demographic groups. This is a common, but potentially controversial, metric.\\n    *   **Equal Opportunity:**  Individuals with similar qualifications should have an equal chance of being selected, regardless of demographic group.  This requires defining \"qualification\" carefully.\\n    *   **Predictive Parity:**  The algorithm\\'s predictions should be equally accurate across different groups. For example, the rate of correctly identifying qualified candidates should be the same for all groups.\\n    *   **Counterfactual Fairness:** An individual should receive the same outcome had they belonged to a different demographic group. This is a more advanced and computationally expensive approach.\\n\\n*   **Define Success Metrics:**  Clearly define what constitutes \"success\" in the role.  These metrics should be measurable and relevant to job performance. They should also be validated to ensure they are not inherently biased (e.g., relying on communication styles that favor certain cultural backgrounds).\\n\\n**2. Data Acquisition & Preprocessing - Focus on Minimizing Bias:**\\n\\n*   **Data Audit:** Conduct a thorough audit of existing historical hiring data. Identify potential sources of bias, such as:\\n    *   **Selection Bias:** The data used to train the algorithm only reflects those who *were* hired, not those who *could have been* successful.\\n    *   **Label Bias:**  Performance evaluations or other labels might be biased, reflecting existing biases in the organization.\\n    *   **Measurement Bias:** The way data is collected or measured might systematically disadvantage certain groups.  (e.g., using tests written primarily for one culture)\\n\\n*   **Data Augmentation/Balancing:**\\n    *   **Oversampling/Undersampling:**  If certain demographic groups are underrepresented in the training data, oversample them or undersample the overrepresented groups.  However, be careful not to introduce synthetic data that perpetuates stereotypes.\\n    *   **Synthetic Data Generation:** Consider creating synthetic data that fills in gaps in the data and mitigates biases, especially when demographic data is sparse.  Use techniques like Generative Adversarial Networks (GANs) carefully to avoid amplifying existing biases.\\n\\n*   **Feature Selection & Engineering:**\\n    *   **Blind the Algorithm:**  Remove or anonymize protected attributes (e.g., gender, race, age, zip code) from the data used to train the model.  However, be aware of *proxy variables* (features that are highly correlated with protected attributes).\\n    *   **Focus on Job-Relevant Skills:**  Prioritize features that directly reflect skills and experience needed for the job, and de-emphasize features that are less relevant or potentially discriminatory (e.g., hobbies, school prestige).\\n    *   **Consider Alternative Credentials:**  Value skills and experience gained through alternative pathways (e.g., online courses, bootcamps, volunteer work) to create a more inclusive talent pool.\\n    *   **Debias Feature Representations:** Use techniques like adversarial debiasing or word embedding debiasing to reduce biases in feature representations, especially when dealing with textual data (e.g., resumes, cover letters).\\n\\n**3. Algorithm Selection & Development:**\\n\\n*   **Choose Appropriate Algorithms:**  Certain algorithms are more prone to bias than others. For example, highly complex models (like deep neural networks) can easily learn and amplify biases present in the data.  Consider using simpler, more transparent models, especially if explainability is important.\\n*   **Regularization Techniques:**  Apply regularization techniques (e.g., L1 or L2 regularization) to prevent the model from overfitting to biased patterns in the data.\\n*   **Adversarial Debiasing:** Train the algorithm to *minimize* its ability to predict protected attributes while *maximizing* its ability to predict job performance.\\n*   **Ensemble Methods:**  Combine multiple models trained on different subsets of the data or with different debiasing techniques to reduce the overall bias.\\n\\n**4. Model Evaluation & Validation - Rigorous Bias Auditing:**\\n\\n*   **Bias Auditing:** Continuously monitor the algorithm\\'s performance for biases across different demographic groups.  Use metrics like:\\n    *   **Disparate Impact:**  Calculate the selection rate for different groups and compare them to see if there is a statistically significant difference.  (Often measured using the 80% rule).\\n    *   **False Positive/Negative Rates:**  Analyze whether the algorithm makes more errors for certain groups (e.g., falsely rejecting qualified candidates from underrepresented groups).\\n    *   **Calibration Analysis:**  Assess whether the algorithm\\'s predicted probabilities accurately reflect the true probability of success for different groups.\\n*   **A/B Testing:**  Compare the performance of the algorithm to a traditional hiring process using A/B testing to evaluate its impact on diversity and inclusion.\\n*   **Explainable AI (XAI):**  Use XAI techniques (e.g., SHAP values, LIME) to understand which features are driving the algorithm\\'s decisions and identify potential sources of bias.  This allows for a more transparent and accountable system.\\n\\n**5. Implementation & Ongoing Monitoring:**\\n\\n*   **Transparency & Explainability:**  Provide applicants with clear and understandable explanations of how the algorithm works and how their application was evaluated.\\n*   **Human Oversight:**  Don\\'t rely solely on the algorithm.  Incorporate human review at key stages of the hiring process to ensure fairness and address any potential biases.  Provide training to hiring managers on how to interpret the algorithm\\'s results and avoid perpetuating biases.\\n*   **Regular Monitoring & Re-Training:**  Continuously monitor the algorithm\\'s performance and re-train it periodically with updated data to ensure it remains fair and accurate.\\n*   **Feedback Mechanisms:**  Establish a process for applicants to provide feedback on the hiring process, including the algorithm.\\n*   **Ethical Review Board:**  Establish an ethical review board to oversee the development and deployment of the algorithm and ensure it aligns with the organization\\'s values.\\n\\n**Important Considerations:**\\n\\n*   **Context Matters:**  The specific techniques used to mitigate bias will depend on the specific job, the data available, and the organization\\'s values.\\n*   **No Perfect Solution:**  It is impossible to eliminate all bias from an algorithm. The goal is to minimize bias as much as possible and ensure that the algorithm is fair and equitable.\\n*   **Legal Compliance:**  Ensure that the algorithm complies with all applicable laws and regulations regarding discrimination.\\n*   **Continuous Improvement:**  Fairness is an ongoing process, not a one-time fix. Continuously monitor the algorithm\\'s performance and adapt it as needed to address any emerging biases.\\n\\nBy following these steps, you can create an algorithm that is more fair and unbiased than a traditional hiring process, and that promotes equitable outcomes for all applicants. It requires a commitment to ethical considerations, data analysis, and ongoing monitoring. Remember to always prioritize fairness and transparency throughout the entire process.\\n', \"Designing a fair and unbiased algorithm for evaluating job applicants requires a multidisciplinary approach, involving expertise in machine learning, psychology, sociology, and ethics. Here's a comprehensive framework to help you create a fair and equitable algorithm:\\n\\n**Pre-design phase**\\n\\n1. **Define the problem and goals**: Identify the specific job requirements, the type of applicants you're looking for, and the desired outcomes. Ensure that the goals are aligned with the organization's values and diversity, equity, and inclusion (DEI) policies.\\n2. **Assemble a diverse team**: Bring together a team with diverse backgrounds, expertise, and perspectives to co-design the algorithm. This team should include data scientists, psychologists, sociologists, ethicists, and HR professionals.\\n3. **Conduct a thorough literature review**: Research existing algorithms, biases, and fairness metrics to understand the current state of the field.\\n\\n**Data collection and preprocessing**\\n\\n1. **Collect diverse and representative data**: Gather data from various sources, including job applications, resumes, cover letters, and assessments. Ensure that the data is representative of the diverse pool of applicants you're targeting.\\n2. **Remove identifiable information**: Anonymize the data by removing identifiable information such as names, ages, addresses, and other demographic characteristics that could introduce biases.\\n3. **Preprocess data**: Clean, transform, and normalize the data to prepare it for analysis.\\n\\n**Algorithm development**\\n\\n1. **Select a suitable algorithm**: Choose an algorithm that is transparent, explainable, and auditable, such as a decision tree or a logistic regression model.\\n2. **Use fairness metrics**: Integrate fairness metrics, such as demographic parity, equal opportunity, or calibration, to evaluate the algorithm's performance and identify potential biases.\\n3. **Regularization techniques**: Implement regularization techniques, such as L1 or L2 regularization, to reduce overfitting and prevent the algorithm from relying too heavily on any single feature.\\n4. **Feature selection**: Select features that are relevant to the job requirements and minimally correlated with protected characteristics (e.g., age, sex, ethnicity).\\n5. **Model interpretability**: Ensure that the algorithm is interpretable, allowing for the identification of features that contribute to the decision-making process.\\n\\n**Bias detection and mitigation**\\n\\n1. **Bias detection tools**: Utilize tools, such as fairness metrics, bias detection algorithms, or audits, to identify potential biases in the algorithm.\\n2. **Debiasing techniques**: Apply debiasing techniques, such as data preprocessing, feature engineering, or regularization, to mitigate identified biases.\\n3. **Human oversight**: Implement human oversight and review processes to detect and correct potential biases.\\n\\n**Evaluation and validation**\\n\\n1. **Evaluate the algorithm**: Assess the algorithm's performance using fairness metrics, accuracy, and other relevant metrics.\\n2. **Validate the results**: Validate the algorithm's results by comparing them to human evaluations or other fairness metrics.\\n3. **Iterate and refine**: Refine the algorithm based on the evaluation and validation results, and iterate until the desired level of fairness and accuracy is achieved.\\n\\n**Deployment and monitoring**\\n\\n1. **Deploy the algorithm**: Deploy the algorithm in a controlled environment, with clear guidelines and protocols for its use.\\n2. **Monitor performance**: Continuously monitor the algorithm's performance, fairness, and accuracy, and update it as needed.\\n3. **Transparency and explainability**: Provide transparency and explainability of the algorithm's decisions, ensuring that applicants and stakeholders understand the evaluation process.\\n\\n**Addressing inherent cognitive biases**\\n\\n1. **Awareness and education**: Educate the development team and users about inherent cognitive biases, such as confirmation bias, anchoring bias, or affinity bias.\\n2. **Design for diversity**: Incorporate diverse perspectives and ideas into the algorithm's design to minimize the impact of cognitive biases.\\n3. **Regular audits**: Conduct regular audits to detect and address potential biases introduced by cognitive biases.\\n\\n**Diverse backgrounds and equitable outcomes**\\n\\n1. **Inclusive design**: Design the algorithm to be inclusive of diverse backgrounds, cultures, and perspectives.\\n2. **Equitable outcomes**: Ensure that the algorithm prioritizes equitable outcomes, such as equal opportunities for underrepresented groups.\\n3. **Continuous improvement**: Continuously monitor and improve the algorithm to ensure that it promotes equitable outcomes and minimizes biases.\\n\\nBy following this framework, you can develop a fair and unbiased algorithm for evaluating job applicants, taking into account inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Remember that fairness and bias are ongoing concerns, and continuous monitoring and improvement are essential to maintaining a fair and equitable algorithm.\", \"Designing a fair and unbiased algorithm for evaluating job applicants requires a multi-faceted approach that addresses inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Here's a step-by-step guide to help you create an algorithm that minimizes bias and ensures fairness:\\n\\n1. **Define clear goals and criteria**: Establish specific evaluation criteria and goals for each position. Ensure these are measurable, objective, and relevant to the job requirements.\\n2. **Data collection and preprocessing**:\\n\\t* Gather data on applicants, including their resumes, cover letters, social media profiles, and any other relevant information.\\n\\t* Preprocess the data by removing irrelevant information, normalizing data formats, and converting text data into numerical representations (e.g., sentiment analysis).\\n3. **Identify bias-reducing techniques**:\\n\\t* Use de-biased resume screening tools that focus on relevant skills and qualifications rather than demographic information.\\n\\t* Implement algorithms that detect and mitigate biases in natural language processing (NLP) tasks, such as sentiment analysis and named entity recognition.\\n4. **Mitigate inherent cognitive biases**:\\n\\t* Use techniques like randomization or stratification to ensure diverse samples of applicants are evaluated.\\n\\t* Implement blind hiring practices, where the algorithm is not trained on identifiable information (e.g., names, ages).\\n5. **Incorporate diversity and inclusion metrics**:\\n\\t* Monitor and track data on applicant diversity, including demographics, education levels, and work experience.\\n\\t* Use algorithms that recognize and reward diversity in the evaluation process.\\n6. **Algorithmic audit and testing**: Regularly review and test your algorithm to ensure it is free from bias and produces equitable results. You can use techniques like:\\n\\t* Bias detection tools (e.g., AI Fairness 360, Demographic Prediction Tool)\\n\\t* Simulations of diverse scenarios to test the algorithm's fairness\\n7. **Regular evaluation and adaptation**:\\n\\t* Continuously monitor the performance of your algorithm and make adjustments as needed.\\n\\t* Include regular review of diversity metrics to ensure the pipeline is fair and inclusive.\\n\\nBest Practices for Developing an Algorithmic Ecosystem:\\n\\n1.  **Engage diverse stakeholders**: Involve HR, diversity experts, and representatives from underrepresented groups in the design and development process.\\n2.  **Establish metrics for fairness**: Use universally accepted metrics (e.g., accuracy metrics) to measure your algorithm's performance and biases.\\n3.  **Regularly review data quality**: Ensure high-quality data is used as input for decision-making.\\n4.  **Implement feedback mechanisms**: Create an open platform for applicants to provide feedback on the hiring process.\\n\\nBy incorporating these best practices, strategies, and algorithms that mitigate bias, you can develop a fair and unbiased algorithm that advances diversity and inclusion in your organization's hiring practices.\\n\\n**Example: AI Model**\\n\\nHere's an algorithm to help evaluate job applicants using AI models:\\n\\n*   **Step 1:** Preprocess application data by normalizing skills, education level, and other relevant information into numerical representations.\\n*   **Step 2:** Train a machine learning (ML) model on the curated preprocessed data, which can identify relevant qualities, skills, and experience in job seekers.\\n*   **Key Metric**:\\n\\n    *   90% accuracy rate for predicting candidate's ability to fit company culture\\n    *   60% diversity bonus when including applicants underrepresented groups\"]\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4o-mini\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants is a multifaceted challenge that requires careful consideration of various aspects, including bias mitigation, representation, and equitable outcomes. Here’s a structured approach to tackle this problem:\n",
      "\n",
      "### 1. **Define Criteria for Evaluation**\n",
      "\n",
      "   - **Identify Job Requirements:** Clearly outline the key competencies, skills, and experiences that are essential for the position. This should be based on data-driven insights and organizational needs.\n",
      "   - **Performance Metrics:** Establish what success looks like in the role, using historical performance data and stakeholder input.\n",
      "\n",
      "### 2. **Data Collection and Representation**\n",
      "\n",
      "   - **Diverse Data Sources:** Collect a comprehensive dataset that includes applicants from various backgrounds. Ensure representation of different demographic groups (gender, race, socioeconomic status, etc.).\n",
      "   - **Bias Audit of Existing Data:** Analyze historical hiring data to identify any inherent biases in past hiring practices. This includes examining patterns that may disadvantage certain groups.\n",
      "\n",
      "### 3. **Bias Mitigation**\n",
      "\n",
      "   - **Blind Recruitment:** Implement anonymized applications where potentially biased information (names, addresses, graduation institutions) is removed to focus on qualifications and experience.\n",
      "   - **Algorithm Fairness Techniques:** Utilize techniques like re-weighting, adversarial debiasing, and fairness constraints to optimize the algorithm for equitable outcomes.\n",
      "   - **Regular Audits:** Establish a schedule for auditing the algorithm’s outcomes for any signs of bias against protected classes.\n",
      "\n",
      "### 4. **Algorithm Design**\n",
      "\n",
      "   - **Explainable AI:** Choose machine learning models that are interpretable. This allows stakeholders to understand decision-making processes and provides insights into potential biases.\n",
      "   - **Multi-modal Input:** Consider various evaluation methods (technical assessments, interviews, role plays) to capture the full range of candidate attributes.\n",
      "\n",
      "### 5. **Incorporating Human Judgment**\n",
      "\n",
      "   - **Hybrid Approach:** Use the algorithm in conjunction with human judgment. Recruiters can have the final say, particularly in culturally sensitive or nuanced evaluations.\n",
      "   - **Training for Recruiters:** Provide training on unconscious bias, diversity, and inclusion, and how to interpret algorithm outputs critically.\n",
      "\n",
      "### 6. **Feedback Loop and Continuous Improvement**\n",
      "\n",
      "   - **Iterative Testing:** Regularly test the algorithm against real-world outcomes and adjust as necessary. Monitor how well the algorithm predicts on-the-job success for different groups.\n",
      "   - **Stakeholder Feedback:** Create channels for feedback from candidates and hiring managers to understand the experience and gather qualitative data.\n",
      "\n",
      "### 7. **Transparency and Accountability**\n",
      "\n",
      "   - **Clear Communication:** Keep all stakeholders informed about how the algorithm is designed, the data it uses, and how decisions are made.\n",
      "   - **Metrics for Success:** Define success metrics that reflect both hiring goals and diversity/inclusion objectives, and report on these metrics publicly.\n",
      "\n",
      "### 8. **Legal and Ethical Considerations**\n",
      "\n",
      "   - **Compliance:** Ensure the algorithm complies with all relevant laws and regulations (e.g., EEOC guidelines in the U.S.) regarding hiring practices and fairness.\n",
      "   - **Ethical Standards:** Establish and adhere to ethical principles guiding the use of AI in hiring, prioritizing fairness, accountability, and transparency.\n",
      "\n",
      "By employing a comprehensive approach that encompasses data integrity, algorithm fairness, human involvement, and continuous improvement, it’s possible to design an innovative and equitable job applicant evaluation system. This process requires collaboration across stakeholders, ongoing assessment, and a commitment to learning from outcomes to continuously refine methods for inclusivity and fairness.\n",
      "Competitor: gemini-2.0-flash\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants is a complex task requiring a multi-faceted approach. Here's a breakdown of how I would tackle it, focusing on minimizing biases and promoting equitable outcomes:\n",
      "\n",
      "**1. Define Fairness & Success Metrics Clearly:**\n",
      "\n",
      "*   **Explicitly Define Fairness:**  The first step is to move beyond vague concepts of fairness and define *what fairness means in this specific context*. This involves stakeholder input (HR, hiring managers, employees, potential applicants).  Examples of fairness definitions include:\n",
      "    *   **Demographic Parity (Statistical Parity):**  The selection rate should be approximately the same for different demographic groups. This is a common, but potentially controversial, metric.\n",
      "    *   **Equal Opportunity:**  Individuals with similar qualifications should have an equal chance of being selected, regardless of demographic group.  This requires defining \"qualification\" carefully.\n",
      "    *   **Predictive Parity:**  The algorithm's predictions should be equally accurate across different groups. For example, the rate of correctly identifying qualified candidates should be the same for all groups.\n",
      "    *   **Counterfactual Fairness:** An individual should receive the same outcome had they belonged to a different demographic group. This is a more advanced and computationally expensive approach.\n",
      "\n",
      "*   **Define Success Metrics:**  Clearly define what constitutes \"success\" in the role.  These metrics should be measurable and relevant to job performance. They should also be validated to ensure they are not inherently biased (e.g., relying on communication styles that favor certain cultural backgrounds).\n",
      "\n",
      "**2. Data Acquisition & Preprocessing - Focus on Minimizing Bias:**\n",
      "\n",
      "*   **Data Audit:** Conduct a thorough audit of existing historical hiring data. Identify potential sources of bias, such as:\n",
      "    *   **Selection Bias:** The data used to train the algorithm only reflects those who *were* hired, not those who *could have been* successful.\n",
      "    *   **Label Bias:**  Performance evaluations or other labels might be biased, reflecting existing biases in the organization.\n",
      "    *   **Measurement Bias:** The way data is collected or measured might systematically disadvantage certain groups.  (e.g., using tests written primarily for one culture)\n",
      "\n",
      "*   **Data Augmentation/Balancing:**\n",
      "    *   **Oversampling/Undersampling:**  If certain demographic groups are underrepresented in the training data, oversample them or undersample the overrepresented groups.  However, be careful not to introduce synthetic data that perpetuates stereotypes.\n",
      "    *   **Synthetic Data Generation:** Consider creating synthetic data that fills in gaps in the data and mitigates biases, especially when demographic data is sparse.  Use techniques like Generative Adversarial Networks (GANs) carefully to avoid amplifying existing biases.\n",
      "\n",
      "*   **Feature Selection & Engineering:**\n",
      "    *   **Blind the Algorithm:**  Remove or anonymize protected attributes (e.g., gender, race, age, zip code) from the data used to train the model.  However, be aware of *proxy variables* (features that are highly correlated with protected attributes).\n",
      "    *   **Focus on Job-Relevant Skills:**  Prioritize features that directly reflect skills and experience needed for the job, and de-emphasize features that are less relevant or potentially discriminatory (e.g., hobbies, school prestige).\n",
      "    *   **Consider Alternative Credentials:**  Value skills and experience gained through alternative pathways (e.g., online courses, bootcamps, volunteer work) to create a more inclusive talent pool.\n",
      "    *   **Debias Feature Representations:** Use techniques like adversarial debiasing or word embedding debiasing to reduce biases in feature representations, especially when dealing with textual data (e.g., resumes, cover letters).\n",
      "\n",
      "**3. Algorithm Selection & Development:**\n",
      "\n",
      "*   **Choose Appropriate Algorithms:**  Certain algorithms are more prone to bias than others. For example, highly complex models (like deep neural networks) can easily learn and amplify biases present in the data.  Consider using simpler, more transparent models, especially if explainability is important.\n",
      "*   **Regularization Techniques:**  Apply regularization techniques (e.g., L1 or L2 regularization) to prevent the model from overfitting to biased patterns in the data.\n",
      "*   **Adversarial Debiasing:** Train the algorithm to *minimize* its ability to predict protected attributes while *maximizing* its ability to predict job performance.\n",
      "*   **Ensemble Methods:**  Combine multiple models trained on different subsets of the data or with different debiasing techniques to reduce the overall bias.\n",
      "\n",
      "**4. Model Evaluation & Validation - Rigorous Bias Auditing:**\n",
      "\n",
      "*   **Bias Auditing:** Continuously monitor the algorithm's performance for biases across different demographic groups.  Use metrics like:\n",
      "    *   **Disparate Impact:**  Calculate the selection rate for different groups and compare them to see if there is a statistically significant difference.  (Often measured using the 80% rule).\n",
      "    *   **False Positive/Negative Rates:**  Analyze whether the algorithm makes more errors for certain groups (e.g., falsely rejecting qualified candidates from underrepresented groups).\n",
      "    *   **Calibration Analysis:**  Assess whether the algorithm's predicted probabilities accurately reflect the true probability of success for different groups.\n",
      "*   **A/B Testing:**  Compare the performance of the algorithm to a traditional hiring process using A/B testing to evaluate its impact on diversity and inclusion.\n",
      "*   **Explainable AI (XAI):**  Use XAI techniques (e.g., SHAP values, LIME) to understand which features are driving the algorithm's decisions and identify potential sources of bias.  This allows for a more transparent and accountable system.\n",
      "\n",
      "**5. Implementation & Ongoing Monitoring:**\n",
      "\n",
      "*   **Transparency & Explainability:**  Provide applicants with clear and understandable explanations of how the algorithm works and how their application was evaluated.\n",
      "*   **Human Oversight:**  Don't rely solely on the algorithm.  Incorporate human review at key stages of the hiring process to ensure fairness and address any potential biases.  Provide training to hiring managers on how to interpret the algorithm's results and avoid perpetuating biases.\n",
      "*   **Regular Monitoring & Re-Training:**  Continuously monitor the algorithm's performance and re-train it periodically with updated data to ensure it remains fair and accurate.\n",
      "*   **Feedback Mechanisms:**  Establish a process for applicants to provide feedback on the hiring process, including the algorithm.\n",
      "*   **Ethical Review Board:**  Establish an ethical review board to oversee the development and deployment of the algorithm and ensure it aligns with the organization's values.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Context Matters:**  The specific techniques used to mitigate bias will depend on the specific job, the data available, and the organization's values.\n",
      "*   **No Perfect Solution:**  It is impossible to eliminate all bias from an algorithm. The goal is to minimize bias as much as possible and ensure that the algorithm is fair and equitable.\n",
      "*   **Legal Compliance:**  Ensure that the algorithm complies with all applicable laws and regulations regarding discrimination.\n",
      "*   **Continuous Improvement:**  Fairness is an ongoing process, not a one-time fix. Continuously monitor the algorithm's performance and adapt it as needed to address any emerging biases.\n",
      "\n",
      "By following these steps, you can create an algorithm that is more fair and unbiased than a traditional hiring process, and that promotes equitable outcomes for all applicants. It requires a commitment to ethical considerations, data analysis, and ongoing monitoring. Remember to always prioritize fairness and transparency throughout the entire process.\n",
      "\n",
      "Competitor: llama-3.3-70b-versatile\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants requires a multidisciplinary approach, involving expertise in machine learning, psychology, sociology, and ethics. Here's a comprehensive framework to help you create a fair and equitable algorithm:\n",
      "\n",
      "**Pre-design phase**\n",
      "\n",
      "1. **Define the problem and goals**: Identify the specific job requirements, the type of applicants you're looking for, and the desired outcomes. Ensure that the goals are aligned with the organization's values and diversity, equity, and inclusion (DEI) policies.\n",
      "2. **Assemble a diverse team**: Bring together a team with diverse backgrounds, expertise, and perspectives to co-design the algorithm. This team should include data scientists, psychologists, sociologists, ethicists, and HR professionals.\n",
      "3. **Conduct a thorough literature review**: Research existing algorithms, biases, and fairness metrics to understand the current state of the field.\n",
      "\n",
      "**Data collection and preprocessing**\n",
      "\n",
      "1. **Collect diverse and representative data**: Gather data from various sources, including job applications, resumes, cover letters, and assessments. Ensure that the data is representative of the diverse pool of applicants you're targeting.\n",
      "2. **Remove identifiable information**: Anonymize the data by removing identifiable information such as names, ages, addresses, and other demographic characteristics that could introduce biases.\n",
      "3. **Preprocess data**: Clean, transform, and normalize the data to prepare it for analysis.\n",
      "\n",
      "**Algorithm development**\n",
      "\n",
      "1. **Select a suitable algorithm**: Choose an algorithm that is transparent, explainable, and auditable, such as a decision tree or a logistic regression model.\n",
      "2. **Use fairness metrics**: Integrate fairness metrics, such as demographic parity, equal opportunity, or calibration, to evaluate the algorithm's performance and identify potential biases.\n",
      "3. **Regularization techniques**: Implement regularization techniques, such as L1 or L2 regularization, to reduce overfitting and prevent the algorithm from relying too heavily on any single feature.\n",
      "4. **Feature selection**: Select features that are relevant to the job requirements and minimally correlated with protected characteristics (e.g., age, sex, ethnicity).\n",
      "5. **Model interpretability**: Ensure that the algorithm is interpretable, allowing for the identification of features that contribute to the decision-making process.\n",
      "\n",
      "**Bias detection and mitigation**\n",
      "\n",
      "1. **Bias detection tools**: Utilize tools, such as fairness metrics, bias detection algorithms, or audits, to identify potential biases in the algorithm.\n",
      "2. **Debiasing techniques**: Apply debiasing techniques, such as data preprocessing, feature engineering, or regularization, to mitigate identified biases.\n",
      "3. **Human oversight**: Implement human oversight and review processes to detect and correct potential biases.\n",
      "\n",
      "**Evaluation and validation**\n",
      "\n",
      "1. **Evaluate the algorithm**: Assess the algorithm's performance using fairness metrics, accuracy, and other relevant metrics.\n",
      "2. **Validate the results**: Validate the algorithm's results by comparing them to human evaluations or other fairness metrics.\n",
      "3. **Iterate and refine**: Refine the algorithm based on the evaluation and validation results, and iterate until the desired level of fairness and accuracy is achieved.\n",
      "\n",
      "**Deployment and monitoring**\n",
      "\n",
      "1. **Deploy the algorithm**: Deploy the algorithm in a controlled environment, with clear guidelines and protocols for its use.\n",
      "2. **Monitor performance**: Continuously monitor the algorithm's performance, fairness, and accuracy, and update it as needed.\n",
      "3. **Transparency and explainability**: Provide transparency and explainability of the algorithm's decisions, ensuring that applicants and stakeholders understand the evaluation process.\n",
      "\n",
      "**Addressing inherent cognitive biases**\n",
      "\n",
      "1. **Awareness and education**: Educate the development team and users about inherent cognitive biases, such as confirmation bias, anchoring bias, or affinity bias.\n",
      "2. **Design for diversity**: Incorporate diverse perspectives and ideas into the algorithm's design to minimize the impact of cognitive biases.\n",
      "3. **Regular audits**: Conduct regular audits to detect and address potential biases introduced by cognitive biases.\n",
      "\n",
      "**Diverse backgrounds and equitable outcomes**\n",
      "\n",
      "1. **Inclusive design**: Design the algorithm to be inclusive of diverse backgrounds, cultures, and perspectives.\n",
      "2. **Equitable outcomes**: Ensure that the algorithm prioritizes equitable outcomes, such as equal opportunities for underrepresented groups.\n",
      "3. **Continuous improvement**: Continuously monitor and improve the algorithm to ensure that it promotes equitable outcomes and minimizes biases.\n",
      "\n",
      "By following this framework, you can develop a fair and unbiased algorithm for evaluating job applicants, taking into account inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Remember that fairness and bias are ongoing concerns, and continuous monitoring and improvement are essential to maintaining a fair and equitable algorithm.\n",
      "Competitor: llama3.2\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants requires a multi-faceted approach that addresses inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Here's a step-by-step guide to help you create an algorithm that minimizes bias and ensures fairness:\n",
      "\n",
      "1. **Define clear goals and criteria**: Establish specific evaluation criteria and goals for each position. Ensure these are measurable, objective, and relevant to the job requirements.\n",
      "2. **Data collection and preprocessing**:\n",
      "\t* Gather data on applicants, including their resumes, cover letters, social media profiles, and any other relevant information.\n",
      "\t* Preprocess the data by removing irrelevant information, normalizing data formats, and converting text data into numerical representations (e.g., sentiment analysis).\n",
      "3. **Identify bias-reducing techniques**:\n",
      "\t* Use de-biased resume screening tools that focus on relevant skills and qualifications rather than demographic information.\n",
      "\t* Implement algorithms that detect and mitigate biases in natural language processing (NLP) tasks, such as sentiment analysis and named entity recognition.\n",
      "4. **Mitigate inherent cognitive biases**:\n",
      "\t* Use techniques like randomization or stratification to ensure diverse samples of applicants are evaluated.\n",
      "\t* Implement blind hiring practices, where the algorithm is not trained on identifiable information (e.g., names, ages).\n",
      "5. **Incorporate diversity and inclusion metrics**:\n",
      "\t* Monitor and track data on applicant diversity, including demographics, education levels, and work experience.\n",
      "\t* Use algorithms that recognize and reward diversity in the evaluation process.\n",
      "6. **Algorithmic audit and testing**: Regularly review and test your algorithm to ensure it is free from bias and produces equitable results. You can use techniques like:\n",
      "\t* Bias detection tools (e.g., AI Fairness 360, Demographic Prediction Tool)\n",
      "\t* Simulations of diverse scenarios to test the algorithm's fairness\n",
      "7. **Regular evaluation and adaptation**:\n",
      "\t* Continuously monitor the performance of your algorithm and make adjustments as needed.\n",
      "\t* Include regular review of diversity metrics to ensure the pipeline is fair and inclusive.\n",
      "\n",
      "Best Practices for Developing an Algorithmic Ecosystem:\n",
      "\n",
      "1.  **Engage diverse stakeholders**: Involve HR, diversity experts, and representatives from underrepresented groups in the design and development process.\n",
      "2.  **Establish metrics for fairness**: Use universally accepted metrics (e.g., accuracy metrics) to measure your algorithm's performance and biases.\n",
      "3.  **Regularly review data quality**: Ensure high-quality data is used as input for decision-making.\n",
      "4.  **Implement feedback mechanisms**: Create an open platform for applicants to provide feedback on the hiring process.\n",
      "\n",
      "By incorporating these best practices, strategies, and algorithms that mitigate bias, you can develop a fair and unbiased algorithm that advances diversity and inclusion in your organization's hiring practices.\n",
      "\n",
      "**Example: AI Model**\n",
      "\n",
      "Here's an algorithm to help evaluate job applicants using AI models:\n",
      "\n",
      "*   **Step 1:** Preprocess application data by normalizing skills, education level, and other relevant information into numerical representations.\n",
      "*   **Step 2:** Train a machine learning (ML) model on the curated preprocessed data, which can identify relevant qualities, skills, and experience in job seekers.\n",
      "*   **Key Metric**:\n",
      "\n",
      "    *   90% accuracy rate for predicting candidate's ability to fit company culture\n",
      "    *   60% diversity bonus when including applicants underrepresented groups\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants is a multifaceted challenge that requires careful consideration of various aspects, including bias mitigation, representation, and equitable outcomes. Here’s a structured approach to tackle this problem:\n",
      "\n",
      "### 1. **Define Criteria for Evaluation**\n",
      "\n",
      "   - **Identify Job Requirements:** Clearly outline the key competencies, skills, and experiences that are essential for the position. This should be based on data-driven insights and organizational needs.\n",
      "   - **Performance Metrics:** Establish what success looks like in the role, using historical performance data and stakeholder input.\n",
      "\n",
      "### 2. **Data Collection and Representation**\n",
      "\n",
      "   - **Diverse Data Sources:** Collect a comprehensive dataset that includes applicants from various backgrounds. Ensure representation of different demographic groups (gender, race, socioeconomic status, etc.).\n",
      "   - **Bias Audit of Existing Data:** Analyze historical hiring data to identify any inherent biases in past hiring practices. This includes examining patterns that may disadvantage certain groups.\n",
      "\n",
      "### 3. **Bias Mitigation**\n",
      "\n",
      "   - **Blind Recruitment:** Implement anonymized applications where potentially biased information (names, addresses, graduation institutions) is removed to focus on qualifications and experience.\n",
      "   - **Algorithm Fairness Techniques:** Utilize techniques like re-weighting, adversarial debiasing, and fairness constraints to optimize the algorithm for equitable outcomes.\n",
      "   - **Regular Audits:** Establish a schedule for auditing the algorithm’s outcomes for any signs of bias against protected classes.\n",
      "\n",
      "### 4. **Algorithm Design**\n",
      "\n",
      "   - **Explainable AI:** Choose machine learning models that are interpretable. This allows stakeholders to understand decision-making processes and provides insights into potential biases.\n",
      "   - **Multi-modal Input:** Consider various evaluation methods (technical assessments, interviews, role plays) to capture the full range of candidate attributes.\n",
      "\n",
      "### 5. **Incorporating Human Judgment**\n",
      "\n",
      "   - **Hybrid Approach:** Use the algorithm in conjunction with human judgment. Recruiters can have the final say, particularly in culturally sensitive or nuanced evaluations.\n",
      "   - **Training for Recruiters:** Provide training on unconscious bias, diversity, and inclusion, and how to interpret algorithm outputs critically.\n",
      "\n",
      "### 6. **Feedback Loop and Continuous Improvement**\n",
      "\n",
      "   - **Iterative Testing:** Regularly test the algorithm against real-world outcomes and adjust as necessary. Monitor how well the algorithm predicts on-the-job success for different groups.\n",
      "   - **Stakeholder Feedback:** Create channels for feedback from candidates and hiring managers to understand the experience and gather qualitative data.\n",
      "\n",
      "### 7. **Transparency and Accountability**\n",
      "\n",
      "   - **Clear Communication:** Keep all stakeholders informed about how the algorithm is designed, the data it uses, and how decisions are made.\n",
      "   - **Metrics for Success:** Define success metrics that reflect both hiring goals and diversity/inclusion objectives, and report on these metrics publicly.\n",
      "\n",
      "### 8. **Legal and Ethical Considerations**\n",
      "\n",
      "   - **Compliance:** Ensure the algorithm complies with all relevant laws and regulations (e.g., EEOC guidelines in the U.S.) regarding hiring practices and fairness.\n",
      "   - **Ethical Standards:** Establish and adhere to ethical principles guiding the use of AI in hiring, prioritizing fairness, accountability, and transparency.\n",
      "\n",
      "By employing a comprehensive approach that encompasses data integrity, algorithm fairness, human involvement, and continuous improvement, it’s possible to design an innovative and equitable job applicant evaluation system. This process requires collaboration across stakeholders, ongoing assessment, and a commitment to learning from outcomes to continuously refine methods for inclusivity and fairness.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants is a complex task requiring a multi-faceted approach. Here's a breakdown of how I would tackle it, focusing on minimizing biases and promoting equitable outcomes:\n",
      "\n",
      "**1. Define Fairness & Success Metrics Clearly:**\n",
      "\n",
      "*   **Explicitly Define Fairness:**  The first step is to move beyond vague concepts of fairness and define *what fairness means in this specific context*. This involves stakeholder input (HR, hiring managers, employees, potential applicants).  Examples of fairness definitions include:\n",
      "    *   **Demographic Parity (Statistical Parity):**  The selection rate should be approximately the same for different demographic groups. This is a common, but potentially controversial, metric.\n",
      "    *   **Equal Opportunity:**  Individuals with similar qualifications should have an equal chance of being selected, regardless of demographic group.  This requires defining \"qualification\" carefully.\n",
      "    *   **Predictive Parity:**  The algorithm's predictions should be equally accurate across different groups. For example, the rate of correctly identifying qualified candidates should be the same for all groups.\n",
      "    *   **Counterfactual Fairness:** An individual should receive the same outcome had they belonged to a different demographic group. This is a more advanced and computationally expensive approach.\n",
      "\n",
      "*   **Define Success Metrics:**  Clearly define what constitutes \"success\" in the role.  These metrics should be measurable and relevant to job performance. They should also be validated to ensure they are not inherently biased (e.g., relying on communication styles that favor certain cultural backgrounds).\n",
      "\n",
      "**2. Data Acquisition & Preprocessing - Focus on Minimizing Bias:**\n",
      "\n",
      "*   **Data Audit:** Conduct a thorough audit of existing historical hiring data. Identify potential sources of bias, such as:\n",
      "    *   **Selection Bias:** The data used to train the algorithm only reflects those who *were* hired, not those who *could have been* successful.\n",
      "    *   **Label Bias:**  Performance evaluations or other labels might be biased, reflecting existing biases in the organization.\n",
      "    *   **Measurement Bias:** The way data is collected or measured might systematically disadvantage certain groups.  (e.g., using tests written primarily for one culture)\n",
      "\n",
      "*   **Data Augmentation/Balancing:**\n",
      "    *   **Oversampling/Undersampling:**  If certain demographic groups are underrepresented in the training data, oversample them or undersample the overrepresented groups.  However, be careful not to introduce synthetic data that perpetuates stereotypes.\n",
      "    *   **Synthetic Data Generation:** Consider creating synthetic data that fills in gaps in the data and mitigates biases, especially when demographic data is sparse.  Use techniques like Generative Adversarial Networks (GANs) carefully to avoid amplifying existing biases.\n",
      "\n",
      "*   **Feature Selection & Engineering:**\n",
      "    *   **Blind the Algorithm:**  Remove or anonymize protected attributes (e.g., gender, race, age, zip code) from the data used to train the model.  However, be aware of *proxy variables* (features that are highly correlated with protected attributes).\n",
      "    *   **Focus on Job-Relevant Skills:**  Prioritize features that directly reflect skills and experience needed for the job, and de-emphasize features that are less relevant or potentially discriminatory (e.g., hobbies, school prestige).\n",
      "    *   **Consider Alternative Credentials:**  Value skills and experience gained through alternative pathways (e.g., online courses, bootcamps, volunteer work) to create a more inclusive talent pool.\n",
      "    *   **Debias Feature Representations:** Use techniques like adversarial debiasing or word embedding debiasing to reduce biases in feature representations, especially when dealing with textual data (e.g., resumes, cover letters).\n",
      "\n",
      "**3. Algorithm Selection & Development:**\n",
      "\n",
      "*   **Choose Appropriate Algorithms:**  Certain algorithms are more prone to bias than others. For example, highly complex models (like deep neural networks) can easily learn and amplify biases present in the data.  Consider using simpler, more transparent models, especially if explainability is important.\n",
      "*   **Regularization Techniques:**  Apply regularization techniques (e.g., L1 or L2 regularization) to prevent the model from overfitting to biased patterns in the data.\n",
      "*   **Adversarial Debiasing:** Train the algorithm to *minimize* its ability to predict protected attributes while *maximizing* its ability to predict job performance.\n",
      "*   **Ensemble Methods:**  Combine multiple models trained on different subsets of the data or with different debiasing techniques to reduce the overall bias.\n",
      "\n",
      "**4. Model Evaluation & Validation - Rigorous Bias Auditing:**\n",
      "\n",
      "*   **Bias Auditing:** Continuously monitor the algorithm's performance for biases across different demographic groups.  Use metrics like:\n",
      "    *   **Disparate Impact:**  Calculate the selection rate for different groups and compare them to see if there is a statistically significant difference.  (Often measured using the 80% rule).\n",
      "    *   **False Positive/Negative Rates:**  Analyze whether the algorithm makes more errors for certain groups (e.g., falsely rejecting qualified candidates from underrepresented groups).\n",
      "    *   **Calibration Analysis:**  Assess whether the algorithm's predicted probabilities accurately reflect the true probability of success for different groups.\n",
      "*   **A/B Testing:**  Compare the performance of the algorithm to a traditional hiring process using A/B testing to evaluate its impact on diversity and inclusion.\n",
      "*   **Explainable AI (XAI):**  Use XAI techniques (e.g., SHAP values, LIME) to understand which features are driving the algorithm's decisions and identify potential sources of bias.  This allows for a more transparent and accountable system.\n",
      "\n",
      "**5. Implementation & Ongoing Monitoring:**\n",
      "\n",
      "*   **Transparency & Explainability:**  Provide applicants with clear and understandable explanations of how the algorithm works and how their application was evaluated.\n",
      "*   **Human Oversight:**  Don't rely solely on the algorithm.  Incorporate human review at key stages of the hiring process to ensure fairness and address any potential biases.  Provide training to hiring managers on how to interpret the algorithm's results and avoid perpetuating biases.\n",
      "*   **Regular Monitoring & Re-Training:**  Continuously monitor the algorithm's performance and re-train it periodically with updated data to ensure it remains fair and accurate.\n",
      "*   **Feedback Mechanisms:**  Establish a process for applicants to provide feedback on the hiring process, including the algorithm.\n",
      "*   **Ethical Review Board:**  Establish an ethical review board to oversee the development and deployment of the algorithm and ensure it aligns with the organization's values.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Context Matters:**  The specific techniques used to mitigate bias will depend on the specific job, the data available, and the organization's values.\n",
      "*   **No Perfect Solution:**  It is impossible to eliminate all bias from an algorithm. The goal is to minimize bias as much as possible and ensure that the algorithm is fair and equitable.\n",
      "*   **Legal Compliance:**  Ensure that the algorithm complies with all applicable laws and regulations regarding discrimination.\n",
      "*   **Continuous Improvement:**  Fairness is an ongoing process, not a one-time fix. Continuously monitor the algorithm's performance and adapt it as needed to address any emerging biases.\n",
      "\n",
      "By following these steps, you can create an algorithm that is more fair and unbiased than a traditional hiring process, and that promotes equitable outcomes for all applicants. It requires a commitment to ethical considerations, data analysis, and ongoing monitoring. Remember to always prioritize fairness and transparency throughout the entire process.\n",
      "\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants requires a multidisciplinary approach, involving expertise in machine learning, psychology, sociology, and ethics. Here's a comprehensive framework to help you create a fair and equitable algorithm:\n",
      "\n",
      "**Pre-design phase**\n",
      "\n",
      "1. **Define the problem and goals**: Identify the specific job requirements, the type of applicants you're looking for, and the desired outcomes. Ensure that the goals are aligned with the organization's values and diversity, equity, and inclusion (DEI) policies.\n",
      "2. **Assemble a diverse team**: Bring together a team with diverse backgrounds, expertise, and perspectives to co-design the algorithm. This team should include data scientists, psychologists, sociologists, ethicists, and HR professionals.\n",
      "3. **Conduct a thorough literature review**: Research existing algorithms, biases, and fairness metrics to understand the current state of the field.\n",
      "\n",
      "**Data collection and preprocessing**\n",
      "\n",
      "1. **Collect diverse and representative data**: Gather data from various sources, including job applications, resumes, cover letters, and assessments. Ensure that the data is representative of the diverse pool of applicants you're targeting.\n",
      "2. **Remove identifiable information**: Anonymize the data by removing identifiable information such as names, ages, addresses, and other demographic characteristics that could introduce biases.\n",
      "3. **Preprocess data**: Clean, transform, and normalize the data to prepare it for analysis.\n",
      "\n",
      "**Algorithm development**\n",
      "\n",
      "1. **Select a suitable algorithm**: Choose an algorithm that is transparent, explainable, and auditable, such as a decision tree or a logistic regression model.\n",
      "2. **Use fairness metrics**: Integrate fairness metrics, such as demographic parity, equal opportunity, or calibration, to evaluate the algorithm's performance and identify potential biases.\n",
      "3. **Regularization techniques**: Implement regularization techniques, such as L1 or L2 regularization, to reduce overfitting and prevent the algorithm from relying too heavily on any single feature.\n",
      "4. **Feature selection**: Select features that are relevant to the job requirements and minimally correlated with protected characteristics (e.g., age, sex, ethnicity).\n",
      "5. **Model interpretability**: Ensure that the algorithm is interpretable, allowing for the identification of features that contribute to the decision-making process.\n",
      "\n",
      "**Bias detection and mitigation**\n",
      "\n",
      "1. **Bias detection tools**: Utilize tools, such as fairness metrics, bias detection algorithms, or audits, to identify potential biases in the algorithm.\n",
      "2. **Debiasing techniques**: Apply debiasing techniques, such as data preprocessing, feature engineering, or regularization, to mitigate identified biases.\n",
      "3. **Human oversight**: Implement human oversight and review processes to detect and correct potential biases.\n",
      "\n",
      "**Evaluation and validation**\n",
      "\n",
      "1. **Evaluate the algorithm**: Assess the algorithm's performance using fairness metrics, accuracy, and other relevant metrics.\n",
      "2. **Validate the results**: Validate the algorithm's results by comparing them to human evaluations or other fairness metrics.\n",
      "3. **Iterate and refine**: Refine the algorithm based on the evaluation and validation results, and iterate until the desired level of fairness and accuracy is achieved.\n",
      "\n",
      "**Deployment and monitoring**\n",
      "\n",
      "1. **Deploy the algorithm**: Deploy the algorithm in a controlled environment, with clear guidelines and protocols for its use.\n",
      "2. **Monitor performance**: Continuously monitor the algorithm's performance, fairness, and accuracy, and update it as needed.\n",
      "3. **Transparency and explainability**: Provide transparency and explainability of the algorithm's decisions, ensuring that applicants and stakeholders understand the evaluation process.\n",
      "\n",
      "**Addressing inherent cognitive biases**\n",
      "\n",
      "1. **Awareness and education**: Educate the development team and users about inherent cognitive biases, such as confirmation bias, anchoring bias, or affinity bias.\n",
      "2. **Design for diversity**: Incorporate diverse perspectives and ideas into the algorithm's design to minimize the impact of cognitive biases.\n",
      "3. **Regular audits**: Conduct regular audits to detect and address potential biases introduced by cognitive biases.\n",
      "\n",
      "**Diverse backgrounds and equitable outcomes**\n",
      "\n",
      "1. **Inclusive design**: Design the algorithm to be inclusive of diverse backgrounds, cultures, and perspectives.\n",
      "2. **Equitable outcomes**: Ensure that the algorithm prioritizes equitable outcomes, such as equal opportunities for underrepresented groups.\n",
      "3. **Continuous improvement**: Continuously monitor and improve the algorithm to ensure that it promotes equitable outcomes and minimizes biases.\n",
      "\n",
      "By following this framework, you can develop a fair and unbiased algorithm for evaluating job applicants, taking into account inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Remember that fairness and bias are ongoing concerns, and continuous monitoring and improvement are essential to maintaining a fair and equitable algorithm.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants requires a multi-faceted approach that addresses inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Here's a step-by-step guide to help you create an algorithm that minimizes bias and ensures fairness:\n",
      "\n",
      "1. **Define clear goals and criteria**: Establish specific evaluation criteria and goals for each position. Ensure these are measurable, objective, and relevant to the job requirements.\n",
      "2. **Data collection and preprocessing**:\n",
      "\t* Gather data on applicants, including their resumes, cover letters, social media profiles, and any other relevant information.\n",
      "\t* Preprocess the data by removing irrelevant information, normalizing data formats, and converting text data into numerical representations (e.g., sentiment analysis).\n",
      "3. **Identify bias-reducing techniques**:\n",
      "\t* Use de-biased resume screening tools that focus on relevant skills and qualifications rather than demographic information.\n",
      "\t* Implement algorithms that detect and mitigate biases in natural language processing (NLP) tasks, such as sentiment analysis and named entity recognition.\n",
      "4. **Mitigate inherent cognitive biases**:\n",
      "\t* Use techniques like randomization or stratification to ensure diverse samples of applicants are evaluated.\n",
      "\t* Implement blind hiring practices, where the algorithm is not trained on identifiable information (e.g., names, ages).\n",
      "5. **Incorporate diversity and inclusion metrics**:\n",
      "\t* Monitor and track data on applicant diversity, including demographics, education levels, and work experience.\n",
      "\t* Use algorithms that recognize and reward diversity in the evaluation process.\n",
      "6. **Algorithmic audit and testing**: Regularly review and test your algorithm to ensure it is free from bias and produces equitable results. You can use techniques like:\n",
      "\t* Bias detection tools (e.g., AI Fairness 360, Demographic Prediction Tool)\n",
      "\t* Simulations of diverse scenarios to test the algorithm's fairness\n",
      "7. **Regular evaluation and adaptation**:\n",
      "\t* Continuously monitor the performance of your algorithm and make adjustments as needed.\n",
      "\t* Include regular review of diversity metrics to ensure the pipeline is fair and inclusive.\n",
      "\n",
      "Best Practices for Developing an Algorithmic Ecosystem:\n",
      "\n",
      "1.  **Engage diverse stakeholders**: Involve HR, diversity experts, and representatives from underrepresented groups in the design and development process.\n",
      "2.  **Establish metrics for fairness**: Use universally accepted metrics (e.g., accuracy metrics) to measure your algorithm's performance and biases.\n",
      "3.  **Regularly review data quality**: Ensure high-quality data is used as input for decision-making.\n",
      "4.  **Implement feedback mechanisms**: Create an open platform for applicants to provide feedback on the hiring process.\n",
      "\n",
      "By incorporating these best practices, strategies, and algorithms that mitigate bias, you can develop a fair and unbiased algorithm that advances diversity and inclusion in your organization's hiring practices.\n",
      "\n",
      "**Example: AI Model**\n",
      "\n",
      "Here's an algorithm to help evaluate job applicants using AI models:\n",
      "\n",
      "*   **Step 1:** Preprocess application data by normalizing skills, education level, and other relevant information into numerical representations.\n",
      "*   **Step 2:** Train a machine learning (ML) model on the curated preprocessed data, which can identify relevant qualities, skills, and experience in job seekers.\n",
      "*   **Key Metric**:\n",
      "\n",
      "    *   90% accuracy rate for predicting candidate's ability to fit company culture\n",
      "    *   60% diversity bonus when including applicants underrepresented groups\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 4 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "How would you approach designing a fair and unbiased algorithm for evaluating job applicants, considering factors like inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants is a multifaceted challenge that requires careful consideration of various aspects, including bias mitigation, representation, and equitable outcomes. Here’s a structured approach to tackle this problem:\n",
      "\n",
      "### 1. **Define Criteria for Evaluation**\n",
      "\n",
      "   - **Identify Job Requirements:** Clearly outline the key competencies, skills, and experiences that are essential for the position. This should be based on data-driven insights and organizational needs.\n",
      "   - **Performance Metrics:** Establish what success looks like in the role, using historical performance data and stakeholder input.\n",
      "\n",
      "### 2. **Data Collection and Representation**\n",
      "\n",
      "   - **Diverse Data Sources:** Collect a comprehensive dataset that includes applicants from various backgrounds. Ensure representation of different demographic groups (gender, race, socioeconomic status, etc.).\n",
      "   - **Bias Audit of Existing Data:** Analyze historical hiring data to identify any inherent biases in past hiring practices. This includes examining patterns that may disadvantage certain groups.\n",
      "\n",
      "### 3. **Bias Mitigation**\n",
      "\n",
      "   - **Blind Recruitment:** Implement anonymized applications where potentially biased information (names, addresses, graduation institutions) is removed to focus on qualifications and experience.\n",
      "   - **Algorithm Fairness Techniques:** Utilize techniques like re-weighting, adversarial debiasing, and fairness constraints to optimize the algorithm for equitable outcomes.\n",
      "   - **Regular Audits:** Establish a schedule for auditing the algorithm’s outcomes for any signs of bias against protected classes.\n",
      "\n",
      "### 4. **Algorithm Design**\n",
      "\n",
      "   - **Explainable AI:** Choose machine learning models that are interpretable. This allows stakeholders to understand decision-making processes and provides insights into potential biases.\n",
      "   - **Multi-modal Input:** Consider various evaluation methods (technical assessments, interviews, role plays) to capture the full range of candidate attributes.\n",
      "\n",
      "### 5. **Incorporating Human Judgment**\n",
      "\n",
      "   - **Hybrid Approach:** Use the algorithm in conjunction with human judgment. Recruiters can have the final say, particularly in culturally sensitive or nuanced evaluations.\n",
      "   - **Training for Recruiters:** Provide training on unconscious bias, diversity, and inclusion, and how to interpret algorithm outputs critically.\n",
      "\n",
      "### 6. **Feedback Loop and Continuous Improvement**\n",
      "\n",
      "   - **Iterative Testing:** Regularly test the algorithm against real-world outcomes and adjust as necessary. Monitor how well the algorithm predicts on-the-job success for different groups.\n",
      "   - **Stakeholder Feedback:** Create channels for feedback from candidates and hiring managers to understand the experience and gather qualitative data.\n",
      "\n",
      "### 7. **Transparency and Accountability**\n",
      "\n",
      "   - **Clear Communication:** Keep all stakeholders informed about how the algorithm is designed, the data it uses, and how decisions are made.\n",
      "   - **Metrics for Success:** Define success metrics that reflect both hiring goals and diversity/inclusion objectives, and report on these metrics publicly.\n",
      "\n",
      "### 8. **Legal and Ethical Considerations**\n",
      "\n",
      "   - **Compliance:** Ensure the algorithm complies with all relevant laws and regulations (e.g., EEOC guidelines in the U.S.) regarding hiring practices and fairness.\n",
      "   - **Ethical Standards:** Establish and adhere to ethical principles guiding the use of AI in hiring, prioritizing fairness, accountability, and transparency.\n",
      "\n",
      "By employing a comprehensive approach that encompasses data integrity, algorithm fairness, human involvement, and continuous improvement, it’s possible to design an innovative and equitable job applicant evaluation system. This process requires collaboration across stakeholders, ongoing assessment, and a commitment to learning from outcomes to continuously refine methods for inclusivity and fairness.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants is a complex task requiring a multi-faceted approach. Here's a breakdown of how I would tackle it, focusing on minimizing biases and promoting equitable outcomes:\n",
      "\n",
      "**1. Define Fairness & Success Metrics Clearly:**\n",
      "\n",
      "*   **Explicitly Define Fairness:**  The first step is to move beyond vague concepts of fairness and define *what fairness means in this specific context*. This involves stakeholder input (HR, hiring managers, employees, potential applicants).  Examples of fairness definitions include:\n",
      "    *   **Demographic Parity (Statistical Parity):**  The selection rate should be approximately the same for different demographic groups. This is a common, but potentially controversial, metric.\n",
      "    *   **Equal Opportunity:**  Individuals with similar qualifications should have an equal chance of being selected, regardless of demographic group.  This requires defining \"qualification\" carefully.\n",
      "    *   **Predictive Parity:**  The algorithm's predictions should be equally accurate across different groups. For example, the rate of correctly identifying qualified candidates should be the same for all groups.\n",
      "    *   **Counterfactual Fairness:** An individual should receive the same outcome had they belonged to a different demographic group. This is a more advanced and computationally expensive approach.\n",
      "\n",
      "*   **Define Success Metrics:**  Clearly define what constitutes \"success\" in the role.  These metrics should be measurable and relevant to job performance. They should also be validated to ensure they are not inherently biased (e.g., relying on communication styles that favor certain cultural backgrounds).\n",
      "\n",
      "**2. Data Acquisition & Preprocessing - Focus on Minimizing Bias:**\n",
      "\n",
      "*   **Data Audit:** Conduct a thorough audit of existing historical hiring data. Identify potential sources of bias, such as:\n",
      "    *   **Selection Bias:** The data used to train the algorithm only reflects those who *were* hired, not those who *could have been* successful.\n",
      "    *   **Label Bias:**  Performance evaluations or other labels might be biased, reflecting existing biases in the organization.\n",
      "    *   **Measurement Bias:** The way data is collected or measured might systematically disadvantage certain groups.  (e.g., using tests written primarily for one culture)\n",
      "\n",
      "*   **Data Augmentation/Balancing:**\n",
      "    *   **Oversampling/Undersampling:**  If certain demographic groups are underrepresented in the training data, oversample them or undersample the overrepresented groups.  However, be careful not to introduce synthetic data that perpetuates stereotypes.\n",
      "    *   **Synthetic Data Generation:** Consider creating synthetic data that fills in gaps in the data and mitigates biases, especially when demographic data is sparse.  Use techniques like Generative Adversarial Networks (GANs) carefully to avoid amplifying existing biases.\n",
      "\n",
      "*   **Feature Selection & Engineering:**\n",
      "    *   **Blind the Algorithm:**  Remove or anonymize protected attributes (e.g., gender, race, age, zip code) from the data used to train the model.  However, be aware of *proxy variables* (features that are highly correlated with protected attributes).\n",
      "    *   **Focus on Job-Relevant Skills:**  Prioritize features that directly reflect skills and experience needed for the job, and de-emphasize features that are less relevant or potentially discriminatory (e.g., hobbies, school prestige).\n",
      "    *   **Consider Alternative Credentials:**  Value skills and experience gained through alternative pathways (e.g., online courses, bootcamps, volunteer work) to create a more inclusive talent pool.\n",
      "    *   **Debias Feature Representations:** Use techniques like adversarial debiasing or word embedding debiasing to reduce biases in feature representations, especially when dealing with textual data (e.g., resumes, cover letters).\n",
      "\n",
      "**3. Algorithm Selection & Development:**\n",
      "\n",
      "*   **Choose Appropriate Algorithms:**  Certain algorithms are more prone to bias than others. For example, highly complex models (like deep neural networks) can easily learn and amplify biases present in the data.  Consider using simpler, more transparent models, especially if explainability is important.\n",
      "*   **Regularization Techniques:**  Apply regularization techniques (e.g., L1 or L2 regularization) to prevent the model from overfitting to biased patterns in the data.\n",
      "*   **Adversarial Debiasing:** Train the algorithm to *minimize* its ability to predict protected attributes while *maximizing* its ability to predict job performance.\n",
      "*   **Ensemble Methods:**  Combine multiple models trained on different subsets of the data or with different debiasing techniques to reduce the overall bias.\n",
      "\n",
      "**4. Model Evaluation & Validation - Rigorous Bias Auditing:**\n",
      "\n",
      "*   **Bias Auditing:** Continuously monitor the algorithm's performance for biases across different demographic groups.  Use metrics like:\n",
      "    *   **Disparate Impact:**  Calculate the selection rate for different groups and compare them to see if there is a statistically significant difference.  (Often measured using the 80% rule).\n",
      "    *   **False Positive/Negative Rates:**  Analyze whether the algorithm makes more errors for certain groups (e.g., falsely rejecting qualified candidates from underrepresented groups).\n",
      "    *   **Calibration Analysis:**  Assess whether the algorithm's predicted probabilities accurately reflect the true probability of success for different groups.\n",
      "*   **A/B Testing:**  Compare the performance of the algorithm to a traditional hiring process using A/B testing to evaluate its impact on diversity and inclusion.\n",
      "*   **Explainable AI (XAI):**  Use XAI techniques (e.g., SHAP values, LIME) to understand which features are driving the algorithm's decisions and identify potential sources of bias.  This allows for a more transparent and accountable system.\n",
      "\n",
      "**5. Implementation & Ongoing Monitoring:**\n",
      "\n",
      "*   **Transparency & Explainability:**  Provide applicants with clear and understandable explanations of how the algorithm works and how their application was evaluated.\n",
      "*   **Human Oversight:**  Don't rely solely on the algorithm.  Incorporate human review at key stages of the hiring process to ensure fairness and address any potential biases.  Provide training to hiring managers on how to interpret the algorithm's results and avoid perpetuating biases.\n",
      "*   **Regular Monitoring & Re-Training:**  Continuously monitor the algorithm's performance and re-train it periodically with updated data to ensure it remains fair and accurate.\n",
      "*   **Feedback Mechanisms:**  Establish a process for applicants to provide feedback on the hiring process, including the algorithm.\n",
      "*   **Ethical Review Board:**  Establish an ethical review board to oversee the development and deployment of the algorithm and ensure it aligns with the organization's values.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Context Matters:**  The specific techniques used to mitigate bias will depend on the specific job, the data available, and the organization's values.\n",
      "*   **No Perfect Solution:**  It is impossible to eliminate all bias from an algorithm. The goal is to minimize bias as much as possible and ensure that the algorithm is fair and equitable.\n",
      "*   **Legal Compliance:**  Ensure that the algorithm complies with all applicable laws and regulations regarding discrimination.\n",
      "*   **Continuous Improvement:**  Fairness is an ongoing process, not a one-time fix. Continuously monitor the algorithm's performance and adapt it as needed to address any emerging biases.\n",
      "\n",
      "By following these steps, you can create an algorithm that is more fair and unbiased than a traditional hiring process, and that promotes equitable outcomes for all applicants. It requires a commitment to ethical considerations, data analysis, and ongoing monitoring. Remember to always prioritize fairness and transparency throughout the entire process.\n",
      "\n",
      "\n",
      "# Response from competitor 3\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants requires a multidisciplinary approach, involving expertise in machine learning, psychology, sociology, and ethics. Here's a comprehensive framework to help you create a fair and equitable algorithm:\n",
      "\n",
      "**Pre-design phase**\n",
      "\n",
      "1. **Define the problem and goals**: Identify the specific job requirements, the type of applicants you're looking for, and the desired outcomes. Ensure that the goals are aligned with the organization's values and diversity, equity, and inclusion (DEI) policies.\n",
      "2. **Assemble a diverse team**: Bring together a team with diverse backgrounds, expertise, and perspectives to co-design the algorithm. This team should include data scientists, psychologists, sociologists, ethicists, and HR professionals.\n",
      "3. **Conduct a thorough literature review**: Research existing algorithms, biases, and fairness metrics to understand the current state of the field.\n",
      "\n",
      "**Data collection and preprocessing**\n",
      "\n",
      "1. **Collect diverse and representative data**: Gather data from various sources, including job applications, resumes, cover letters, and assessments. Ensure that the data is representative of the diverse pool of applicants you're targeting.\n",
      "2. **Remove identifiable information**: Anonymize the data by removing identifiable information such as names, ages, addresses, and other demographic characteristics that could introduce biases.\n",
      "3. **Preprocess data**: Clean, transform, and normalize the data to prepare it for analysis.\n",
      "\n",
      "**Algorithm development**\n",
      "\n",
      "1. **Select a suitable algorithm**: Choose an algorithm that is transparent, explainable, and auditable, such as a decision tree or a logistic regression model.\n",
      "2. **Use fairness metrics**: Integrate fairness metrics, such as demographic parity, equal opportunity, or calibration, to evaluate the algorithm's performance and identify potential biases.\n",
      "3. **Regularization techniques**: Implement regularization techniques, such as L1 or L2 regularization, to reduce overfitting and prevent the algorithm from relying too heavily on any single feature.\n",
      "4. **Feature selection**: Select features that are relevant to the job requirements and minimally correlated with protected characteristics (e.g., age, sex, ethnicity).\n",
      "5. **Model interpretability**: Ensure that the algorithm is interpretable, allowing for the identification of features that contribute to the decision-making process.\n",
      "\n",
      "**Bias detection and mitigation**\n",
      "\n",
      "1. **Bias detection tools**: Utilize tools, such as fairness metrics, bias detection algorithms, or audits, to identify potential biases in the algorithm.\n",
      "2. **Debiasing techniques**: Apply debiasing techniques, such as data preprocessing, feature engineering, or regularization, to mitigate identified biases.\n",
      "3. **Human oversight**: Implement human oversight and review processes to detect and correct potential biases.\n",
      "\n",
      "**Evaluation and validation**\n",
      "\n",
      "1. **Evaluate the algorithm**: Assess the algorithm's performance using fairness metrics, accuracy, and other relevant metrics.\n",
      "2. **Validate the results**: Validate the algorithm's results by comparing them to human evaluations or other fairness metrics.\n",
      "3. **Iterate and refine**: Refine the algorithm based on the evaluation and validation results, and iterate until the desired level of fairness and accuracy is achieved.\n",
      "\n",
      "**Deployment and monitoring**\n",
      "\n",
      "1. **Deploy the algorithm**: Deploy the algorithm in a controlled environment, with clear guidelines and protocols for its use.\n",
      "2. **Monitor performance**: Continuously monitor the algorithm's performance, fairness, and accuracy, and update it as needed.\n",
      "3. **Transparency and explainability**: Provide transparency and explainability of the algorithm's decisions, ensuring that applicants and stakeholders understand the evaluation process.\n",
      "\n",
      "**Addressing inherent cognitive biases**\n",
      "\n",
      "1. **Awareness and education**: Educate the development team and users about inherent cognitive biases, such as confirmation bias, anchoring bias, or affinity bias.\n",
      "2. **Design for diversity**: Incorporate diverse perspectives and ideas into the algorithm's design to minimize the impact of cognitive biases.\n",
      "3. **Regular audits**: Conduct regular audits to detect and address potential biases introduced by cognitive biases.\n",
      "\n",
      "**Diverse backgrounds and equitable outcomes**\n",
      "\n",
      "1. **Inclusive design**: Design the algorithm to be inclusive of diverse backgrounds, cultures, and perspectives.\n",
      "2. **Equitable outcomes**: Ensure that the algorithm prioritizes equitable outcomes, such as equal opportunities for underrepresented groups.\n",
      "3. **Continuous improvement**: Continuously monitor and improve the algorithm to ensure that it promotes equitable outcomes and minimizes biases.\n",
      "\n",
      "By following this framework, you can develop a fair and unbiased algorithm for evaluating job applicants, taking into account inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Remember that fairness and bias are ongoing concerns, and continuous monitoring and improvement are essential to maintaining a fair and equitable algorithm.\n",
      "\n",
      "# Response from competitor 4\n",
      "\n",
      "Designing a fair and unbiased algorithm for evaluating job applicants requires a multi-faceted approach that addresses inherent cognitive biases, diverse backgrounds, and the need for equitable outcomes. Here's a step-by-step guide to help you create an algorithm that minimizes bias and ensures fairness:\n",
      "\n",
      "1. **Define clear goals and criteria**: Establish specific evaluation criteria and goals for each position. Ensure these are measurable, objective, and relevant to the job requirements.\n",
      "2. **Data collection and preprocessing**:\n",
      "\t* Gather data on applicants, including their resumes, cover letters, social media profiles, and any other relevant information.\n",
      "\t* Preprocess the data by removing irrelevant information, normalizing data formats, and converting text data into numerical representations (e.g., sentiment analysis).\n",
      "3. **Identify bias-reducing techniques**:\n",
      "\t* Use de-biased resume screening tools that focus on relevant skills and qualifications rather than demographic information.\n",
      "\t* Implement algorithms that detect and mitigate biases in natural language processing (NLP) tasks, such as sentiment analysis and named entity recognition.\n",
      "4. **Mitigate inherent cognitive biases**:\n",
      "\t* Use techniques like randomization or stratification to ensure diverse samples of applicants are evaluated.\n",
      "\t* Implement blind hiring practices, where the algorithm is not trained on identifiable information (e.g., names, ages).\n",
      "5. **Incorporate diversity and inclusion metrics**:\n",
      "\t* Monitor and track data on applicant diversity, including demographics, education levels, and work experience.\n",
      "\t* Use algorithms that recognize and reward diversity in the evaluation process.\n",
      "6. **Algorithmic audit and testing**: Regularly review and test your algorithm to ensure it is free from bias and produces equitable results. You can use techniques like:\n",
      "\t* Bias detection tools (e.g., AI Fairness 360, Demographic Prediction Tool)\n",
      "\t* Simulations of diverse scenarios to test the algorithm's fairness\n",
      "7. **Regular evaluation and adaptation**:\n",
      "\t* Continuously monitor the performance of your algorithm and make adjustments as needed.\n",
      "\t* Include regular review of diversity metrics to ensure the pipeline is fair and inclusive.\n",
      "\n",
      "Best Practices for Developing an Algorithmic Ecosystem:\n",
      "\n",
      "1.  **Engage diverse stakeholders**: Involve HR, diversity experts, and representatives from underrepresented groups in the design and development process.\n",
      "2.  **Establish metrics for fairness**: Use universally accepted metrics (e.g., accuracy metrics) to measure your algorithm's performance and biases.\n",
      "3.  **Regularly review data quality**: Ensure high-quality data is used as input for decision-making.\n",
      "4.  **Implement feedback mechanisms**: Create an open platform for applicants to provide feedback on the hiring process.\n",
      "\n",
      "By incorporating these best practices, strategies, and algorithms that mitigate bias, you can develop a fair and unbiased algorithm that advances diversity and inclusion in your organization's hiring practices.\n",
      "\n",
      "**Example: AI Model**\n",
      "\n",
      "Here's an algorithm to help evaluate job applicants using AI models:\n",
      "\n",
      "*   **Step 1:** Preprocess application data by normalizing skills, education level, and other relevant information into numerical representations.\n",
      "*   **Step 2:** Train a machine learning (ML) model on the curated preprocessed data, which can identify relevant qualities, skills, and experience in job seekers.\n",
      "*   **Key Metric**:\n",
      "\n",
      "    *   90% accuracy rate for predicting candidate's ability to fit company culture\n",
      "    *   60% diversity bonus when including applicants underrepresented groups\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"2\", \"1\", \"3\", \"4\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gemini-2.0-flash\n",
      "Rank 2: gpt-4o-mini\n",
      "Rank 3: llama-3.3-70b-versatile\n",
      "Rank 4: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
